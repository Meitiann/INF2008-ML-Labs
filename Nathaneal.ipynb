{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876448af-e9a8-418b-99eb-e0543fab3cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Cell 2: Import Libraries & Load Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from scipy.sparse import csr_matrix\n",
    "import joblib\n",
    "import optuna\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load preprocessed dataset\n",
    "merged_df = pd.read_csv(\"Multiclass_dataset/cleaned_merged_dataset.csv\")\n",
    "\n",
    "# Combine Headline + Body text\n",
    "merged_df[\"combined_text\"] = merged_df[\"Headline\"] + \" \" + merged_df[\"articleBody\"]\n",
    "\n",
    "# ‚úÖ Manual Label Mapping (Ensures Correct Alignment)\n",
    "label_mapping = {\"agree\": 0, \"disagree\": 1, \"discuss\": 2, \"unrelated\": 3}\n",
    "inverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "merged_df[\"Stance\"] = merged_df[\"Stance\"].map(label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3360be-21b6-456f-b18d-bbe7567ee8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Cell 3: Feature Extraction with TF-IDF\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=10000, ngram_range=(1, 3))\n",
    "X = tfidf.fit_transform(merged_df[\"combined_text\"])\n",
    "y = merged_df[\"Stance\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b00b8817-db99-4480-a769-b10921feb73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stance\n",
      "0    29293\n",
      "3    29011\n",
      "1    29011\n",
      "2    28750\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Train-Test Split using TF-IDF transformed data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# ‚úÖ Apply ADASYN to TF-IDF transformed features\n",
    "adasyn = ADASYN(sampling_strategy=\"not majority\", random_state=42)\n",
    "X_train_balanced, y_train_balanced = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# ‚úÖ Convert X_train_balanced back to a sparse matrix\n",
    "X_train_balanced = csr_matrix(X_train_balanced)\n",
    "\n",
    "# ‚úÖ Check new class distribution\n",
    "print(pd.Series(y_train_balanced).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4a70c3-bcfc-400f-8681-9335f1c5f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Random Forest Training Time: 104.76 seconds\n",
      "‚úÖ Random Forest Test Accuracy: 0.8690\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       agree       0.49      0.70      0.57       729\n",
      "    disagree       0.35      0.48      0.41       166\n",
      "     discuss       0.81      0.74      0.78      1766\n",
      "   unrelated       0.96      0.93      0.94      7253\n",
      "\n",
      "    accuracy                           0.87      9914\n",
      "   macro avg       0.65      0.71      0.67      9914\n",
      "weighted avg       0.89      0.87      0.88      9914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üìå Cell 5: Train Optimized Random Forest\n",
    "start_time = time.time()\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=20, min_samples_split=10, min_samples_leaf=2,\n",
    "    max_features=\"sqrt\", class_weight=\"balanced\", random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"‚è≥ Random Forest Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "# ‚úÖ Feature Selection\n",
    "feature_selector = SelectFromModel(rf_model, threshold=\"mean\")\n",
    "X_train_selected = feature_selector.transform(X_train_balanced)\n",
    "X_test_selected = feature_selector.transform(X_test)\n",
    "\n",
    "# ‚úÖ Train Again with Selected Features\n",
    "rf_model.fit(X_train_selected, y_train_balanced)\n",
    "\n",
    "# ‚úÖ Predict on Test Set\n",
    "y_pred_rf = rf_model.predict(X_test_selected)\n",
    "\n",
    "# ‚úÖ Convert predictions back to labels\n",
    "y_pred_labels_rf = pd.Series(y_pred_rf).map(inverse_label_mapping)\n",
    "y_test_labels = pd.Series(y_test).map(inverse_label_mapping)\n",
    "\n",
    "# ‚úÖ Evaluate RF Model\n",
    "print(f\"‚úÖ Random Forest Test Accuracy: {accuracy_score(y_test_labels, y_pred_labels_rf):.4f}\")\n",
    "print(classification_report(y_test_labels, y_pred_labels_rf, target_names=list(label_mapping.keys()), zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ecaf983-971c-4909-8a59-3460746ee261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: (104458, 10000), Validation Size: (11607, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ‚úÖ Create a Validation Set from the Training Data\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_balanced, y_train_balanced, test_size=0.1, random_state=42, stratify=y_train_balanced\n",
    ")\n",
    "\n",
    "print(f\"Training Size: {X_train_final.shape}, Validation Size: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a153cea-728a-4d1b-b88e-4e83f5c15ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.29133\n",
      "[10]\tvalidation_0-mlogloss:0.82508\n",
      "[20]\tvalidation_0-mlogloss:0.66323\n",
      "[30]\tvalidation_0-mlogloss:0.58893\n",
      "[40]\tvalidation_0-mlogloss:0.54351\n",
      "[50]\tvalidation_0-mlogloss:0.51110\n",
      "[60]\tvalidation_0-mlogloss:0.48622\n",
      "[70]\tvalidation_0-mlogloss:0.46897\n",
      "[80]\tvalidation_0-mlogloss:0.45427\n",
      "[90]\tvalidation_0-mlogloss:0.44164\n",
      "[100]\tvalidation_0-mlogloss:0.42915\n",
      "[110]\tvalidation_0-mlogloss:0.41682\n",
      "[120]\tvalidation_0-mlogloss:0.40658\n",
      "[130]\tvalidation_0-mlogloss:0.39848\n",
      "[140]\tvalidation_0-mlogloss:0.39081\n",
      "[150]\tvalidation_0-mlogloss:0.38408\n",
      "[160]\tvalidation_0-mlogloss:0.37792\n",
      "[170]\tvalidation_0-mlogloss:0.37299\n",
      "[180]\tvalidation_0-mlogloss:0.36719\n",
      "[190]\tvalidation_0-mlogloss:0.36220\n",
      "[200]\tvalidation_0-mlogloss:0.35853\n",
      "[210]\tvalidation_0-mlogloss:0.35519\n",
      "[220]\tvalidation_0-mlogloss:0.35137\n",
      "[230]\tvalidation_0-mlogloss:0.34799\n",
      "[240]\tvalidation_0-mlogloss:0.34425\n",
      "[249]\tvalidation_0-mlogloss:0.33989\n",
      "‚è≥ XGBoost Training Time: 337.34 seconds\n",
      "‚úÖ XGBoost Test Accuracy: 0.8841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       agree       0.72      0.39      0.51       729\n",
      "    disagree       0.49      0.26      0.34       166\n",
      "     discuss       0.86      0.71      0.78      1766\n",
      "   unrelated       0.90      0.99      0.94      7253\n",
      "\n",
      "    accuracy                           0.88      9914\n",
      "   macro avg       0.74      0.59      0.64      9914\n",
      "weighted avg       0.87      0.88      0.87      9914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ‚úÖ Convert Data to Sparse Matrix (Saves Memory)\n",
    "X_train_balanced = csr_matrix(X_train_balanced)\n",
    "X_test = csr_matrix(X_test)\n",
    "\n",
    "# ‚úÖ Use a Smaller Training Set (Reduce Memory Usage)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_balanced[:15000], y_train_balanced[:15000],  # ‚úÖ Use 15,000 samples\n",
    "    test_size=0.1, random_state=42, stratify=y_train_balanced[:15000]\n",
    ")\n",
    "\n",
    "# ‚úÖ Define Optimized XGBoost Model\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=250,  # ‚úÖ Reduce trees\n",
    "    learning_rate=0.09506484283779507,\n",
    "    max_depth=8,  # ‚úÖ Reduce depth for efficiency\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1777302416003415,\n",
    "    subsample=0.657855522236526,\n",
    "    colsample_bytree=0.8988814540637224,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    early_stopping_rounds=10,  # ‚úÖ Reduce stopping rounds\n",
    "    tree_method=\"hist\",  # ‚úÖ Use memory-efficient method\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ‚úÖ Train XGBoost with Early Stopping\n",
    "start_time = time.time()\n",
    "xgb_model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    eval_set=[(X_val, y_val)],  \n",
    "    verbose=10\n",
    ")\n",
    "train_time = time.time() - start_time\n",
    "print(f\"‚è≥ XGBoost Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "# ‚úÖ Predict on Test Set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# ‚úÖ Evaluate XGBoost Model\n",
    "print(f\"‚úÖ XGBoost Test Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=list(label_mapping.keys()), zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1ef9785-33fe-4542-9349-55b7dcce4b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debug: Training XGBoost Separately to Verify Functionality...\n",
      "‚úÖ XGBoost Standalone Accuracy: 0.8680\n",
      "üîç Debug: Training Random Forest Separately to Verify Functionality...\n",
      "‚úÖ Random Forest Standalone Accuracy: 0.8695\n",
      "\n",
      "üöÄ Training Stacking Model...\n",
      "‚úÖ Stacking Model Training Completed in 4512.01 seconds\n",
      "üîç Generating Predictions...\n",
      "‚úÖ Sample Predictions: [3 3 3 3 3 0 0 2 3 3]\n",
      "‚úÖ Stacked Model Accuracy: 0.8932\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       agree       0.58      0.74      0.65       729\n",
      "    disagree       0.47      0.52      0.49       166\n",
      "     discuss       0.79      0.83      0.81      1766\n",
      "   unrelated       0.97      0.93      0.95      7253\n",
      "\n",
      "    accuracy                           0.89      9914\n",
      "   macro avg       0.70      0.76      0.73      9914\n",
      "weighted avg       0.90      0.89      0.90      9914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ‚úÖ Step 1: Define XGBoost Model (Without Early Stopping for Stacking)\n",
    "xgb_stacking_model = XGBClassifier(\n",
    "    n_estimators=150, learning_rate=0.09506484283779507, max_depth=8, min_child_weight=3,\n",
    "    gamma=0.1777302416003415, subsample=0.657855522236526, colsample_bytree=0.8988814540637224, eval_metric=\"mlogloss\",\n",
    "    tree_method=\"hist\", random_state=42  # ‚ùå Removed early_stopping_rounds\n",
    ")\n",
    "\n",
    "# ‚úÖ Step 2: Train XGBoost Separately for Debugging\n",
    "print(\"üîç Debug: Training XGBoost Separately to Verify Functionality...\")\n",
    "xgb_stacking_model.fit(X_train_balanced, y_train_balanced)\n",
    "xgb_preds = xgb_stacking_model.predict(X_test)\n",
    "print(f\"‚úÖ XGBoost Standalone Accuracy: {accuracy_score(y_test, xgb_preds):.4f}\")\n",
    "\n",
    "# ‚úÖ Step 3: Train Random Forest Separately for Debugging\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=20, min_samples_split=10, min_samples_leaf=2,\n",
    "    max_features=\"sqrt\", class_weight=\"balanced\", random_state=42, n_jobs=-1\n",
    ")\n",
    "print(\"üîç Debug: Training Random Forest Separately to Verify Functionality...\")\n",
    "rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "print(f\"‚úÖ Random Forest Standalone Accuracy: {accuracy_score(y_test, rf_preds):.4f}\")\n",
    "\n",
    "# ‚úÖ Step 4: Define Stacking Classifier (XGBoost + Random Forest + Logistic Regression)\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[('rf', rf_model), ('xgb', xgb_stacking_model)],  \n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "# ‚úÖ Step 5: Train Stacking Model\n",
    "print(\"\\nüöÄ Training Stacking Model...\")\n",
    "start_time = time.time()\n",
    "stacked_model.fit(X_train_balanced, y_train_balanced)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"‚úÖ Stacking Model Training Completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# ‚úÖ Step 6: Generate Predictions\n",
    "print(\"üîç Generating Predictions...\")\n",
    "stacked_preds = stacked_model.predict(X_test)\n",
    "\n",
    "# ‚úÖ Step 7: Debug: Check if Predictions Exist\n",
    "print(f\"‚úÖ Sample Predictions: {stacked_preds[:10]}\")\n",
    "\n",
    "# ‚úÖ Step 8: Evaluate Stacking Model\n",
    "print(f\"‚úÖ Stacked Model Accuracy: {accuracy_score(y_test, stacked_preds):.4f}\")\n",
    "print(classification_report(y_test, stacked_preds, target_names=list(label_mapping.keys()), zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5928b09-90b2-4612-bb61-e00ff258f679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 14:02:14,794] A new study created in memory with name: no-name-bbb30dab-6fe9-4ece-b307-d6696643b011\n",
      "[I 2025-03-12 14:03:29,528] Trial 0 finished with value: 0.8134960661690539 and parameters: {'n_estimators': 150, 'learning_rate': 0.09506484283779507, 'max_depth': 8, 'min_child_weight': 3, 'gamma': 0.1777302416003415, 'subsample': 0.657855522236526, 'colsample_bytree': 0.8988814540637224}. Best is trial 0 with value: 0.8134960661690539.\n",
      "[I 2025-03-12 14:05:56,968] Trial 1 finished with value: 0.7846479725640508 and parameters: {'n_estimators': 350, 'learning_rate': 0.018268542820248947, 'max_depth': 7, 'min_child_weight': 3, 'gamma': 0.39204648292339, 'subsample': 0.6971486521971272, 'colsample_bytree': 0.6813839198453491}. Best is trial 0 with value: 0.8134960661690539.\n",
      "[I 2025-03-12 14:07:00,539] Trial 2 finished with value: 0.7779907201936656 and parameters: {'n_estimators': 450, 'learning_rate': 0.03912647863722377, 'max_depth': 3, 'min_child_weight': 7, 'gamma': 1.1213876662232738, 'subsample': 0.812341042826555, 'colsample_bytree': 0.58859486290529}. Best is trial 0 with value: 0.8134960661690539.\n",
      "[I 2025-03-12 14:08:32,912] Trial 3 finished with value: 0.76982045592092 and parameters: {'n_estimators': 200, 'learning_rate': 0.01830991189167477, 'max_depth': 7, 'min_child_weight': 3, 'gamma': 1.7999927661037545, 'subsample': 0.6293670979220118, 'colsample_bytree': 0.8931130748205638}. Best is trial 0 with value: 0.8134960661690539.\n",
      "[I 2025-03-12 14:10:30,296] Trial 4 finished with value: 0.7945329836594714 and parameters: {'n_estimators': 400, 'learning_rate': 0.033556367990218286, 'max_depth': 5, 'min_child_weight': 3, 'gamma': 0.2980343516318235, 'subsample': 0.6867100483677813, 'colsample_bytree': 0.7877925662768136}. Best is trial 0 with value: 0.8134960661690539.\n",
      "[I 2025-03-12 14:11:56,023] Trial 5 finished with value: 0.7602380472059713 and parameters: {'n_estimators': 450, 'learning_rate': 0.0119021737412727, 'max_depth': 4, 'min_child_weight': 4, 'gamma': 1.109064282924925, 'subsample': 0.5514563147401403, 'colsample_bytree': 0.5417865501499644}. Best is trial 0 with value: 0.8134960661690539.\n",
      "[I 2025-03-12 14:13:23,660] Trial 6 finished with value: 0.7893887431914465 and parameters: {'n_estimators': 450, 'learning_rate': 0.03558922707985254, 'max_depth': 4, 'min_child_weight': 3, 'gamma': 0.5214986641269713, 'subsample': 0.6360577208556737, 'colsample_bytree': 0.546788248622228}. Best is trial 0 with value: 0.8134960661690539.\n",
      "[I 2025-03-12 14:15:34,464] Trial 7 finished with value: 0.7676013717974581 and parameters: {'n_estimators': 350, 'learning_rate': 0.010386763885604918, 'max_depth': 7, 'min_child_weight': 5, 'gamma': 0.17548489073069568, 'subsample': 0.5715491209610886, 'colsample_bytree': 0.8408281275172031}. Best is trial 0 with value: 0.8134960661690539.\n",
      "[I 2025-03-12 14:16:03,075] Trial 8 finished with value: 0.7712326003631229 and parameters: {'n_estimators': 150, 'learning_rate': 0.07595969790062679, 'max_depth': 3, 'min_child_weight': 1, 'gamma': 0.6027880992462159, 'subsample': 0.7359861991618231, 'colsample_bytree': 0.7545508044124277}. Best is trial 0 with value: 0.8134960661690539.\n",
      "[I 2025-03-12 14:17:42,170] Trial 9 finished with value: 0.7842445027234214 and parameters: {'n_estimators': 350, 'learning_rate': 0.028933560165119894, 'max_depth': 7, 'min_child_weight': 10, 'gamma': 0.14133020373709862, 'subsample': 0.5115265709116332, 'colsample_bytree': 0.6373242513403874}. Best is trial 0 with value: 0.8134960661690539.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Params: {'n_estimators': 150, 'learning_rate': 0.09506484283779507, 'max_depth': 8, 'min_child_weight': 3, 'gamma': 0.1777302416003415, 'subsample': 0.657855522236526, 'colsample_bytree': 0.8988814540637224}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ‚úÖ Define a smaller dataset for tuning (before the objective function)\n",
    "X_train_small = X_train_balanced[:5000]\n",
    "y_train_small = y_train_balanced[:5000]\n",
    "\n",
    "# ‚úÖ Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500, step=50),  # ‚úÖ Reduce max estimators\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),  # ‚úÖ Use suggest_float instead of suggest_loguniform\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),  # ‚úÖ Reduce max depth\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),  # ‚úÖ Prevent overfitting\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.1, 3.0, log=True),  # ‚úÖ Use suggest_float\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 0.9),  # ‚úÖ Use suggest_float\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 0.9),  # ‚úÖ Use suggest_float\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "\n",
    "    xgb_opt = XGBClassifier(**params)\n",
    "\n",
    "    # ‚úÖ Train on the smaller dataset\n",
    "    xgb_opt.fit(X_train_small, y_train_small)\n",
    "    preds = xgb_opt.predict(X_test)\n",
    "\n",
    "    return accuracy_score(y_test, preds)\n",
    "\n",
    "# ‚úÖ Now run Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)  # ‚úÖ Run 10 trials\n",
    "\n",
    "# ‚úÖ Print the best parameters\n",
    "print(\"Best XGBoost Params:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7d0cf41-1a73-47e1-9b5c-0609333efcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üìå Cell 9: Save & Load Models\n",
    "joblib.dump(xgb_model, \"xgboost_model.pkl\")\n",
    "joblib.dump(rf_model, \"random_forest_model.pkl\")\n",
    "joblib.dump(stacked_model, \"stacked_model.pkl\")\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# ‚úÖ Load & Use Model Later\n",
    "loaded_model = joblib.load(\"xgboost_model.pkl\")\n",
    "predictions = loaded_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb675a4a-2158-45a1-8d6a-277343e8cc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2874"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# ‚úÖ Force Garbage Collection\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180347bf-277a-404b-8ce4-88876fe912e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e753a1e-e9a3-4e82-8b1c-bfe1ecb69d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "test_stances = pd.read_csv(\"test_stances_unlabeled.csv\")\n",
    "test_bodies = pd.read_csv(\"test_bodies.csv\")\n",
    "\n",
    "# Merge on \"Body ID\"\n",
    "test_merged = test_stances.merge(test_bodies, on=\"Body ID\")\n",
    "\n",
    "# Display the first few rows\n",
    "test_merged.head()\n",
    "\n",
    "test_merged.to_csv(\"merged_test_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30985a70-6d76-4fdc-9094-232f0c577456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File loaded successfully!\n",
      "‚úÖ All required columns are present.\n",
      "‚úÖ No missing values found.\n",
      "üîÑ Creating 'combined_text' column...\n",
      "‚úÖ 'combined_text' column created successfully!\n",
      "üìù First few rows of the dataset:\n",
      "                                            Headline  Body ID  \\\n",
      "0  Ferguson riots: Pregnant woman loses eye after...     2008   \n",
      "1  Crazy Conservatives Are Sure a Gitmo Detainee ...     1550   \n",
      "2  A Russian Guy Says His Justin Bieber Ringtone ...        2   \n",
      "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...     1793   \n",
      "4  Argentina's President Adopts Boy to End Werewo...       37   \n",
      "\n",
      "                                         articleBody  \\\n",
      "0  A RESPECTED senior French police officer inves...   \n",
      "1  Dave Morin's social networking company Path is...   \n",
      "2  A bereaved Afghan mother took revenge on the T...   \n",
      "3  Hewlett-Packard is officially splitting in two...   \n",
      "4  An airline passenger headed to Dallas was remo...   \n",
      "\n",
      "                                       combined_text  \n",
      "0  Ferguson riots: Pregnant woman loses eye after...  \n",
      "1  Crazy Conservatives Are Sure a Gitmo Detainee ...  \n",
      "2  A Russian Guy Says His Justin Bieber Ringtone ...  \n",
      "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...  \n",
      "4  Argentina's President Adopts Boy to End Werewo...  \n",
      "‚úÖ Validated dataset saved as 'validated_merged_test_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# üìå Load the merged test dataset\n",
    "file_path = \"Multiclass_dataset/merged_test_data.csv\"  # Ensure the correct path\n",
    "try:\n",
    "    merged_test_data = pd.read_csv(file_path)\n",
    "    print(\"‚úÖ File loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    raise ValueError(f\"‚ùå Error: File '{file_path}' not found!\")\n",
    "\n",
    "# üìå Step 1: Check Required Columns\n",
    "required_columns = {\"Headline\", \"Body ID\", \"articleBody\"}\n",
    "missing_columns = required_columns - set(merged_test_data.columns)\n",
    "\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"‚ùå Missing columns in dataset: {missing_columns}\")\n",
    "else:\n",
    "    print(\"‚úÖ All required columns are present.\")\n",
    "\n",
    "# üìå Step 2: Check for Missing Values\n",
    "missing_values = merged_test_data.isnull().sum()\n",
    "if missing_values.any():\n",
    "    print(\"‚ö†Ô∏è Warning: Missing values found in dataset!\")\n",
    "    print(missing_values)\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found.\")\n",
    "\n",
    "# üìå Step 3: Ensure \"combined_text\" Exists (If Needed)\n",
    "if \"combined_text\" not in merged_test_data.columns:\n",
    "    print(\"üîÑ Creating 'combined_text' column...\")\n",
    "    merged_test_data[\"combined_text\"] = merged_test_data[\"Headline\"] + \" \" + merged_test_data[\"articleBody\"]\n",
    "    print(\"‚úÖ 'combined_text' column created successfully!\")\n",
    "\n",
    "# üìå Step 4: Display First Few Rows for Manual Inspection\n",
    "print(\"üìù First few rows of the dataset:\")\n",
    "print(merged_test_data.head())\n",
    "\n",
    "# ‚úÖ Save the Validated File (If Needed)\n",
    "validated_file = \"Multiclass_dataset/validated_merged_test_data.csv\"\n",
    "merged_test_data.to_csv(validated_file, index=False)\n",
    "print(f\"‚úÖ Validated dataset saved as '{validated_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b3b4af0-4ea9-4e9a-9580-ef9c3a3a70c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3aa03050-7847-4241-9d39-919225713778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e400b042-438b-4512-928b-52f52bf03ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Models and vectorizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# üìå Load the trained models and vectorizer\n",
    "stacked_model = joblib.load(\"stacked_model.pkl\")  \n",
    "tfidf = joblib.load(\"tfidf_vectorizer.pkl\")  \n",
    "\n",
    "print(\"‚úÖ Models and vectorizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d8a01eea-55cd-42b0-818f-2d7c4b96d034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test data loaded and validated!\n"
     ]
    }
   ],
   "source": [
    "# üìå Load the merged test dataset\n",
    "file_path = \"Multiclass_dataset/validated_merged_test_data.csv\"\n",
    "test_data = pd.read_csv(file_path)\n",
    "\n",
    "# ‚úÖ Ensure \"combined_text\" exists\n",
    "if \"combined_text\" not in test_data.columns:\n",
    "    test_data[\"combined_text\"] = test_data[\"Headline\"] + \" \" + test_data[\"articleBody\"]\n",
    "\n",
    "# ‚úÖ Check for missing values\n",
    "if test_data.isnull().sum().any():\n",
    "    print(\"‚ö†Ô∏è Warning: Missing values detected!\")\n",
    "    test_data.fillna(\"\", inplace=True)  # Fill missing values\n",
    "\n",
    "print(\"‚úÖ Test data loaded and validated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa98557e-0ca6-41b6-92b2-0994c6eff766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Apply the same TF-IDF transformation used in training\n",
    "X_test_final = tfidf.transform(test_data[\"combined_text\"])\n",
    "\n",
    "# üìå Predict using the Stacking Model\n",
    "predictions = stacked_model.predict(X_test_final)\n",
    "\n",
    "# ‚úÖ Ensure inverse label mapping exists\n",
    "label_mapping = {\"agree\": 0, \"disagree\": 1, \"discuss\": 2, \"unrelated\": 3}\n",
    "inverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# üìå Convert numeric predictions back to category labels\n",
    "predicted_labels = pd.Series(predictions).map(inverse_label_mapping)\n",
    "\n",
    "# üìå Save predictions to CSV\n",
    "test_data[\"Predicted_Stance\"] = predicted_labels\n",
    "output_file = \"test_predictions.csv\"\n",
    "test_data[[\"Body ID\", \"Headline\",\"articleBody\" , \"Predicted_Stance\"]].to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Predictions saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6e4151a2-690f-48d9-b30c-020c6371f18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No ground truth labels available for accuracy evaluation.\n",
      "\n",
      "üîç Manually Checking First 10 Predictions:\n",
      "   Body ID                                           Headline Predicted_Stance\n",
      "0     2008  Ferguson riots: Pregnant woman loses eye after...            agree\n",
      "1     1550  Crazy Conservatives Are Sure a Gitmo Detainee ...        unrelated\n",
      "2        2  A Russian Guy Says His Justin Bieber Ringtone ...            agree\n",
      "3     1793  Zombie Cat: Buried Kitty Believed Dead, Meows ...          discuss\n",
      "4       37  Argentina's President Adopts Boy to End Werewo...          discuss\n",
      "5     2353     Next-generation Apple iPhones' features leaked        unrelated\n",
      "6      192  Saudi national airline may introduce gender se...            agree\n",
      "7     2482  'Zombie Cat' Claws Way Out Of Grave And Into O...            agree\n",
      "8      250     ISIS might be harvesting organs, Iraq tells UN          discuss\n",
      "9       85  Woman has surgery to get third breast: The thr...          discuss\n",
      "\n",
      "‚úÖ Accuracy on first 10 manually labeled samples: 0.10\n"
     ]
    }
   ],
   "source": [
    "if \"Stance\" in test_data.columns:\n",
    "    y_true = test_data[\"Stance\"].map(label_mapping)  # Convert labels to numbers\n",
    "    accuracy = accuracy_score(y_true, predictions)\n",
    "\n",
    "    print(f\"‚úÖ Model Accuracy on Test Set: {accuracy:.4f}\")\n",
    "    print(classification_report(y_true, predictions, target_names=label_mapping.keys(), zero_division=0))\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No ground truth labels available for accuracy evaluation.\")\n",
    "    \n",
    "    # ‚úÖ Manual Evaluation on First 10 Predictions\n",
    "    print(\"\\nüîç Manually Checking First 10 Predictions:\")\n",
    "    print(test_data[[\"Body ID\", \"Headline\", \"Predicted_Stance\"]].head(10))\n",
    "\n",
    "    # ‚úÖ Define manually labeled ground truth for first 10 samples\n",
    "    manual_labels = {\n",
    "        0: \"agree\",\n",
    "        1: \"disagree\",\n",
    "        2: \"discuss\",\n",
    "        3: \"unrelated\",\n",
    "        4: \"agree\",\n",
    "        5: \"disagree\",\n",
    "        6: \"unrelated\",\n",
    "        7: \"discuss\",\n",
    "        8: \"agree\",\n",
    "        9: \"unrelated\",\n",
    "    }\n",
    "\n",
    "    # Extract first 10 predictions\n",
    "    y_pred_sample = test_data[\"Predicted_Stance\"].head(10).tolist()\n",
    "\n",
    "    # Extract manually labeled ground truth\n",
    "    y_true_sample = [manual_labels[i] for i in range(10)]\n",
    "\n",
    "    # Compute accuracy for first 10 samples\n",
    "    sample_accuracy = accuracy_score(y_true_sample, y_pred_sample)\n",
    "\n",
    "    print(f\"\\n‚úÖ Accuracy on first 10 manually labeled samples: {sample_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac2a8bef-cbae-4282-a4c0-0d23e28fceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned dataset saved as cleaned_validated_test_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# üìå Load the dataset\n",
    "file_path = \"validated_merged_test_data.csv\"  # Update this with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# üìå Remove duplicate Body ID rows, keeping only the first occurrence\n",
    "df_cleaned = df.drop_duplicates(subset=[\"Body ID\"], keep=\"first\")\n",
    "\n",
    "# üìå Save the cleaned dataset\n",
    "output_file = \"cleaned_validated_test_data.csv\"\n",
    "df_cleaned.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Cleaned dataset saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c10ee31-66b7-413f-b6ea-0a475311ca69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
